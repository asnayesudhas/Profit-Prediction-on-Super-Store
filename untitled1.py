# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DTmH0suJYcpVTxP5Af9jT6WcLOBChWvK
"""

import pandas as pd

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

data=pd.read_csv("superstoressss.csv")
print(data.head())

data.shape

print(data.isnull().sum())

data=data.drop_duplicates()
print(data)

data.shape

data=data.drop_duplicates()
print(data)

num_cols=data.select_dtypes(include=['int64','float64']).columns
cat_cols=data.select_dtypes(include=['object']).columns
num_imputer=SimpleImputer(strategy='mean')
cat_imputer=SimpleImputer(strategy='most_frequent')
data[num_cols]=num_imputer.fit_transform(data[num_cols])
data[cat_cols]=cat_imputer.fit_transform(data[cat_cols])
print(num_cols)
print(cat_cols)

def remove_ouliers(data,columns):
  Q1=data[columns].quanile(0.25)
  Q3=data[columns].quanile(0.75)
  IQR=Q3-Q1
  lower=Q1-1.5*IQR
  upper=Q3+1.5*IQR
  return data[(data[columns]>=lower) &(data[columns]<=upper)]
  data=remove_outliers(data,"Fare")

scaler=StandardScaler()
data[num_cols]=scaler.fit_transform(data[num_cols])

from sklearn.preprocessing import LabelEncoder

# Create a copy of the dataframe to encode categorical columns for correlation calculation
data_encoded =data.copy()

# Encode categorical columns to numerical values
for col in cat_cols:
    le = LabelEncoder()
    data_encoded[col] = le.fit_transform(data_encoded[col])

# Now calculate correlation on the encoded dataframe
corr = data_encoded.corr()
plt.figure(dpi=130)
sns.heatmap(corr,annot=True,fmt='.2f',cmap='coolwarm')
plt.show()
print(corr['Profit'].sort_values(ascending=False))

profit_status = data['Profit'].apply(lambda x: 'Profit_level' if x > 0 else 'Loss_level')
profit_counts = profit_status.value_counts()

plt.pie(profit_counts, labels=profit_counts.index, autopct='%.f%%', shadow=True)
plt.title('Profit Distribution')
plt.show()

X = data_encoded.drop('Profit', axis=1)

y = data_encoded['Profit']

X = StandardScaler().fit(X).transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.3, random_state = 4)

print ('Train set:', X_train.shape, y_train.shape)
print ('Test set:', X_test.shape, y_test.shape)

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import r2_score
print("RÂ² Score:", r2_score(y_test, y_pred))